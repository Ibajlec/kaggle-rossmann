{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=1-NYPQw5THU&feature=youtu.be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from pandas_summary import DataFrameSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jschwindt/anaconda3/envs/itba/lib/python3.6/site-packages/pyarrow/pandas_compat.py:752: FutureWarning: .labels was deprecated in version 0.24.0. Use .codes instead.\n",
      "  labels, = index.labels\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_feather('train_normalized_data.fth')\n",
    "df_test = pd.read_feather('test_normalized_data.fth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen', 'Promo2Weeks', \n",
    "            'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear', 'State', \n",
    "            'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw', 'SchoolHoliday_fw', 'SchoolHoliday_bw']\n",
    "\n",
    "# cat_vars = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'State']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "contin_vars = ['CompetitionDistance', \n",
    "   'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n",
    "   'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n",
    "   'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n",
    "   'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday']\n",
    "# contin_vars = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out_columns = ['Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad en val: 30188, porcentaje: 0.9642465458145908\n"
     ]
    }
   ],
   "source": [
    "df_train = df[df.Date < datetime.datetime(2015, 7, 1)]  \n",
    "df_val = df[df.Date >= datetime.datetime(2015, 7, 1)]\n",
    "print(f'Cantidad en val: {len(df_val)}, porcentaje: {len(df_train)/(len(df_train) + len(df_val))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[cat_vars + contin_vars]\n",
    "X_val = df_val[cat_vars + contin_vars]\n",
    "X_test = df_test[cat_vars + contin_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((814150, 38), (30188, 38))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_output = True\n",
    "    \n",
    "if log_output:\n",
    "    # Escala logaritmica\n",
    "    max_log_y = np.max(np.log(df[y_out_columns])).values\n",
    "    y_train = np.log(df_train[y_out_columns].values)/max_log_y\n",
    "    y_val = np.log(df_val[y_out_columns].values)/max_log_y\n",
    "else:\n",
    "    # NormalizaciÃ³n\n",
    "    y_mean = df_train[y_out_columns].mean().values\n",
    "    y_std = df_train[y_out_columns].std().values\n",
    "    y_train = (df_train[y_out_columns].values - y_mean)/y_std\n",
    "    y_val = (df_val[y_out_columns].values - y_mean)/y_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_child_samples=5\n",
    "n_estimators=4000\n",
    "learning_rate=0.05\n",
    "model = XGBRegressor(min_child_samples=min_child_samples, n_estimators=n_estimators, learning_rate=learning_rate )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params={\"early_stopping_rounds\":100, \n",
    "            \"eval_metric\" : 'rmse', \n",
    "            \"eval_set\" : [(X_val, y_val.reshape(-1))],\n",
    "            'verbose': 100,\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:49:29] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[0]\tvalidation_0-rmse:0.311556\n",
      "Will train until validation_0-rmse hasn't improved in 100 rounds.\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train.reshape(-1), **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "\n",
    "    def score(self, X_val, y_val):\n",
    "        assert(min(y_val) > 0)\n",
    "        guessed_sales = self.guess(X_val)\n",
    "        relative_err = np.absolute((y_val - guessed_sales) / y_val)\n",
    "        result = np.sum(relative_err) / len(y_val)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoost(Model):\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        super().__init__()\n",
    "        dtrain = xgb.DMatrix(X_train, label=np.log(y_train))\n",
    "        evallist = [(dtrain, 'train')]\n",
    "        param = {'nthread': -1,\n",
    "                 'max_depth': 7,\n",
    "                 'eta': 0.02,\n",
    "                 'silent': 1,\n",
    "                 'objective': 'reg:linear',\n",
    "                 'colsample_bytree': 0.7,\n",
    "                 'subsample': 0.7}\n",
    "        num_round = 3000\n",
    "        self.bst = xgb.train(param, dtrain, num_round, evallist)\n",
    "        print(\"Result on validation data: \", self.evaluate(X_val, y_val))\n",
    "\n",
    "    def guess(self, feature):\n",
    "        dtest = xgb.DMatrix(feature)\n",
    "        return numpy.exp(self.bst.predict(dtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:0.683309\n",
      "[1]\ttrain-rmse:0.669704\n",
      "[2]\ttrain-rmse:0.656355\n",
      "[3]\ttrain-rmse:0.643276\n",
      "[4]\ttrain-rmse:0.630462\n",
      "[5]\ttrain-rmse:0.617914\n",
      "[6]\ttrain-rmse:0.60561\n",
      "[7]\ttrain-rmse:0.593558\n",
      "[8]\ttrain-rmse:0.581742\n",
      "[9]\ttrain-rmse:0.570161\n",
      "[10]\ttrain-rmse:0.558811\n",
      "[11]\ttrain-rmse:0.547697\n",
      "[12]\ttrain-rmse:0.536808\n",
      "[13]\ttrain-rmse:0.526125\n",
      "[14]\ttrain-rmse:0.515674\n",
      "[15]\ttrain-rmse:0.505418\n",
      "[16]\ttrain-rmse:0.49537\n",
      "[17]\ttrain-rmse:0.485532\n",
      "[18]\ttrain-rmse:0.475891\n",
      "[19]\ttrain-rmse:0.466437\n",
      "[20]\ttrain-rmse:0.457177\n",
      "[21]\ttrain-rmse:0.448105\n",
      "[22]\ttrain-rmse:0.439215\n",
      "[23]\ttrain-rmse:0.430506\n",
      "[24]\ttrain-rmse:0.421965\n",
      "[25]\ttrain-rmse:0.4136\n",
      "[26]\ttrain-rmse:0.405402\n",
      "[27]\ttrain-rmse:0.397372\n",
      "[28]\ttrain-rmse:0.389493\n",
      "[29]\ttrain-rmse:0.381786\n",
      "[30]\ttrain-rmse:0.374228\n",
      "[31]\ttrain-rmse:0.366823\n",
      "[32]\ttrain-rmse:0.359565\n",
      "[33]\ttrain-rmse:0.352457\n",
      "[34]\ttrain-rmse:0.345487\n",
      "[35]\ttrain-rmse:0.338662\n",
      "[36]\ttrain-rmse:0.331976\n",
      "[37]\ttrain-rmse:0.325426\n",
      "[38]\ttrain-rmse:0.319004\n",
      "[39]\ttrain-rmse:0.312716\n",
      "[40]\ttrain-rmse:0.306551\n",
      "[41]\ttrain-rmse:0.300514\n",
      "[42]\ttrain-rmse:0.294603\n",
      "[43]\ttrain-rmse:0.288808\n",
      "[44]\ttrain-rmse:0.283132\n",
      "[45]\ttrain-rmse:0.277568\n",
      "[46]\ttrain-rmse:0.272119\n",
      "[47]\ttrain-rmse:0.266782\n",
      "[48]\ttrain-rmse:0.261558\n",
      "[49]\ttrain-rmse:0.256429\n",
      "[50]\ttrain-rmse:0.251403\n",
      "[51]\ttrain-rmse:0.246483\n",
      "[52]\ttrain-rmse:0.241668\n",
      "[53]\ttrain-rmse:0.236948\n",
      "[54]\ttrain-rmse:0.232325\n",
      "[55]\ttrain-rmse:0.227803\n",
      "[56]\ttrain-rmse:0.223367\n",
      "[57]\ttrain-rmse:0.219028\n",
      "[58]\ttrain-rmse:0.214774\n",
      "[59]\ttrain-rmse:0.210598\n",
      "[60]\ttrain-rmse:0.206516\n",
      "[61]\ttrain-rmse:0.202519\n",
      "[62]\ttrain-rmse:0.198605\n",
      "[63]\ttrain-rmse:0.194766\n",
      "[64]\ttrain-rmse:0.191011\n",
      "[65]\ttrain-rmse:0.187333\n",
      "[66]\ttrain-rmse:0.183729\n",
      "[67]\ttrain-rmse:0.180202\n",
      "[68]\ttrain-rmse:0.176748\n",
      "[69]\ttrain-rmse:0.173356\n",
      "[70]\ttrain-rmse:0.170041\n",
      "[71]\ttrain-rmse:0.166791\n",
      "[72]\ttrain-rmse:0.163605\n",
      "[73]\ttrain-rmse:0.160492\n",
      "[74]\ttrain-rmse:0.157443\n",
      "[75]\ttrain-rmse:0.154459\n",
      "[76]\ttrain-rmse:0.15154\n",
      "[77]\ttrain-rmse:0.148674\n",
      "[78]\ttrain-rmse:0.145873\n",
      "[79]\ttrain-rmse:0.14313\n",
      "[80]\ttrain-rmse:0.140449\n",
      "[81]\ttrain-rmse:0.137826\n",
      "[82]\ttrain-rmse:0.135251\n",
      "[83]\ttrain-rmse:0.132726\n",
      "[84]\ttrain-rmse:0.130259\n",
      "[85]\ttrain-rmse:0.127849\n",
      "[86]\ttrain-rmse:0.125494\n",
      "[87]\ttrain-rmse:0.123187\n",
      "[88]\ttrain-rmse:0.120935\n",
      "[89]\ttrain-rmse:0.118722\n",
      "[90]\ttrain-rmse:0.116565\n",
      "[91]\ttrain-rmse:0.114452\n",
      "[92]\ttrain-rmse:0.112369\n",
      "[93]\ttrain-rmse:0.110337\n",
      "[94]\ttrain-rmse:0.108352\n",
      "[95]\ttrain-rmse:0.106412\n",
      "[96]\ttrain-rmse:0.104524\n",
      "[97]\ttrain-rmse:0.102674\n",
      "[98]\ttrain-rmse:0.100863\n",
      "[99]\ttrain-rmse:0.099088\n",
      "[100]\ttrain-rmse:0.097355\n",
      "[101]\ttrain-rmse:0.095665\n",
      "[102]\ttrain-rmse:0.094009\n",
      "[103]\ttrain-rmse:0.092384\n",
      "[104]\ttrain-rmse:0.0908\n",
      "[105]\ttrain-rmse:0.089248\n",
      "[106]\ttrain-rmse:0.087732\n",
      "[107]\ttrain-rmse:0.086253\n",
      "[108]\ttrain-rmse:0.08481\n",
      "[109]\ttrain-rmse:0.083399\n",
      "[110]\ttrain-rmse:0.082001\n",
      "[111]\ttrain-rmse:0.080662\n",
      "[112]\ttrain-rmse:0.079353\n",
      "[113]\ttrain-rmse:0.078041\n",
      "[114]\ttrain-rmse:0.07679\n",
      "[115]\ttrain-rmse:0.075559\n",
      "[116]\ttrain-rmse:0.074353\n",
      "[117]\ttrain-rmse:0.073172\n",
      "[118]\ttrain-rmse:0.072022\n",
      "[119]\ttrain-rmse:0.070909\n",
      "[120]\ttrain-rmse:0.069822\n",
      "[121]\ttrain-rmse:0.06876\n",
      "[122]\ttrain-rmse:0.067702\n",
      "[123]\ttrain-rmse:0.066691\n",
      "[124]\ttrain-rmse:0.065707\n",
      "[125]\ttrain-rmse:0.064751\n",
      "[126]\ttrain-rmse:0.06382\n",
      "[127]\ttrain-rmse:0.062892\n",
      "[128]\ttrain-rmse:0.061982\n",
      "[129]\ttrain-rmse:0.06112\n",
      "[130]\ttrain-rmse:0.060271\n",
      "[131]\ttrain-rmse:0.059438\n",
      "[132]\ttrain-rmse:0.05864\n",
      "[133]\ttrain-rmse:0.057845\n",
      "[134]\ttrain-rmse:0.057046\n",
      "[135]\ttrain-rmse:0.056303\n",
      "[136]\ttrain-rmse:0.055587\n",
      "[137]\ttrain-rmse:0.05489\n",
      "[138]\ttrain-rmse:0.054207\n",
      "[139]\ttrain-rmse:0.053549\n",
      "[140]\ttrain-rmse:0.052901\n",
      "[141]\ttrain-rmse:0.052275\n",
      "[142]\ttrain-rmse:0.051639\n",
      "[143]\ttrain-rmse:0.051002\n",
      "[144]\ttrain-rmse:0.050428\n",
      "[145]\ttrain-rmse:0.049827\n",
      "[146]\ttrain-rmse:0.049279\n",
      "[147]\ttrain-rmse:0.048726\n",
      "[148]\ttrain-rmse:0.048154\n",
      "[149]\ttrain-rmse:0.04763\n",
      "[150]\ttrain-rmse:0.047145\n",
      "[151]\ttrain-rmse:0.046635\n",
      "[152]\ttrain-rmse:0.046179\n",
      "[153]\ttrain-rmse:0.045738\n",
      "[154]\ttrain-rmse:0.04531\n",
      "[155]\ttrain-rmse:0.044893\n",
      "[156]\ttrain-rmse:0.044477\n",
      "[157]\ttrain-rmse:0.044068\n",
      "[158]\ttrain-rmse:0.043632\n",
      "[159]\ttrain-rmse:0.043254\n",
      "[160]\ttrain-rmse:0.042895\n",
      "[161]\ttrain-rmse:0.042538\n",
      "[162]\ttrain-rmse:0.0422\n",
      "[163]\ttrain-rmse:0.041859\n",
      "[164]\ttrain-rmse:0.041485\n",
      "[165]\ttrain-rmse:0.041179\n",
      "[166]\ttrain-rmse:0.040873\n",
      "[167]\ttrain-rmse:0.040528\n",
      "[168]\ttrain-rmse:0.040224\n",
      "[169]\ttrain-rmse:0.039925\n",
      "[170]\ttrain-rmse:0.039658\n",
      "[171]\ttrain-rmse:0.039374\n",
      "[172]\ttrain-rmse:0.039073\n",
      "[173]\ttrain-rmse:0.038825\n",
      "[174]\ttrain-rmse:0.038598\n",
      "[175]\ttrain-rmse:0.03831\n",
      "[176]\ttrain-rmse:0.038078\n",
      "[177]\ttrain-rmse:0.037841\n",
      "[178]\ttrain-rmse:0.037636\n",
      "[179]\ttrain-rmse:0.037405\n",
      "[180]\ttrain-rmse:0.03721\n",
      "[181]\ttrain-rmse:0.037\n",
      "[182]\ttrain-rmse:0.036821\n",
      "[183]\ttrain-rmse:0.036607\n",
      "[184]\ttrain-rmse:0.036419\n",
      "[185]\ttrain-rmse:0.036247\n",
      "[186]\ttrain-rmse:0.036035\n",
      "[187]\ttrain-rmse:0.035875\n",
      "[188]\ttrain-rmse:0.035692\n",
      "[189]\ttrain-rmse:0.035507\n",
      "[190]\ttrain-rmse:0.035371\n",
      "[191]\ttrain-rmse:0.035234\n",
      "[192]\ttrain-rmse:0.035101\n",
      "[193]\ttrain-rmse:0.034957\n",
      "[194]\ttrain-rmse:0.034834\n",
      "[195]\ttrain-rmse:0.03471\n",
      "[196]\ttrain-rmse:0.034583\n",
      "[197]\ttrain-rmse:0.034472\n",
      "[198]\ttrain-rmse:0.034339\n",
      "[199]\ttrain-rmse:0.034213\n",
      "[200]\ttrain-rmse:0.034055\n",
      "[201]\ttrain-rmse:0.033963\n",
      "[202]\ttrain-rmse:0.033868\n",
      "[203]\ttrain-rmse:0.033769\n",
      "[204]\ttrain-rmse:0.033668\n",
      "[205]\ttrain-rmse:0.033579\n",
      "[206]\ttrain-rmse:0.033455\n",
      "[207]\ttrain-rmse:0.033366\n",
      "[208]\ttrain-rmse:0.033254\n",
      "[209]\ttrain-rmse:0.033174\n",
      "[210]\ttrain-rmse:0.033082\n",
      "[211]\ttrain-rmse:0.033018\n",
      "[212]\ttrain-rmse:0.032898\n",
      "[213]\ttrain-rmse:0.032804\n",
      "[214]\ttrain-rmse:0.032723\n",
      "[215]\ttrain-rmse:0.032667\n",
      "[216]\ttrain-rmse:0.03259\n",
      "[217]\ttrain-rmse:0.032534\n",
      "[218]\ttrain-rmse:0.03243\n",
      "[219]\ttrain-rmse:0.032379\n",
      "[220]\ttrain-rmse:0.032308\n",
      "[221]\ttrain-rmse:0.032225\n",
      "[222]\ttrain-rmse:0.032163\n",
      "[223]\ttrain-rmse:0.03211\n",
      "[224]\ttrain-rmse:0.032064\n",
      "[225]\ttrain-rmse:0.032012\n",
      "[226]\ttrain-rmse:0.031966\n",
      "[227]\ttrain-rmse:0.031907\n",
      "[228]\ttrain-rmse:0.031862\n",
      "[229]\ttrain-rmse:0.031824\n",
      "[230]\ttrain-rmse:0.031732\n",
      "[231]\ttrain-rmse:0.031647\n",
      "[232]\ttrain-rmse:0.031594\n",
      "[233]\ttrain-rmse:0.031549\n",
      "[234]\ttrain-rmse:0.031517\n",
      "[235]\ttrain-rmse:0.031458\n",
      "[236]\ttrain-rmse:0.031415\n",
      "[237]\ttrain-rmse:0.031371\n",
      "[238]\ttrain-rmse:0.031342\n",
      "[239]\ttrain-rmse:0.031275\n",
      "[240]\ttrain-rmse:0.031245\n",
      "[241]\ttrain-rmse:0.031188\n",
      "[242]\ttrain-rmse:0.031145\n",
      "[243]\ttrain-rmse:0.031074\n",
      "[244]\ttrain-rmse:0.031046\n",
      "[245]\ttrain-rmse:0.031\n",
      "[246]\ttrain-rmse:0.030972\n",
      "[247]\ttrain-rmse:0.030948\n",
      "[248]\ttrain-rmse:0.03091\n",
      "[249]\ttrain-rmse:0.030886\n",
      "[250]\ttrain-rmse:0.030863\n",
      "[251]\ttrain-rmse:0.030835\n",
      "[252]\ttrain-rmse:0.030789\n",
      "[253]\ttrain-rmse:0.030734\n",
      "[254]\ttrain-rmse:0.030703\n",
      "[255]\ttrain-rmse:0.030665\n",
      "[256]\ttrain-rmse:0.030624\n",
      "[257]\ttrain-rmse:0.030591\n",
      "[258]\ttrain-rmse:0.03054\n",
      "[259]\ttrain-rmse:0.030498\n",
      "[260]\ttrain-rmse:0.030485\n",
      "[261]\ttrain-rmse:0.030467\n",
      "[262]\ttrain-rmse:0.030446\n",
      "[263]\ttrain-rmse:0.030383\n",
      "[264]\ttrain-rmse:0.030357\n",
      "[265]\ttrain-rmse:0.030341\n",
      "[266]\ttrain-rmse:0.030292\n",
      "[267]\ttrain-rmse:0.030266\n",
      "[268]\ttrain-rmse:0.030223\n",
      "[269]\ttrain-rmse:0.03017\n",
      "[270]\ttrain-rmse:0.030154\n",
      "[271]\ttrain-rmse:0.030143\n",
      "[272]\ttrain-rmse:0.03009\n",
      "[273]\ttrain-rmse:0.030077\n",
      "[274]\ttrain-rmse:0.030063\n",
      "[275]\ttrain-rmse:0.030036\n",
      "[276]\ttrain-rmse:0.030001\n",
      "[277]\ttrain-rmse:0.029972\n",
      "[278]\ttrain-rmse:0.029928\n",
      "[279]\ttrain-rmse:0.029892\n",
      "[280]\ttrain-rmse:0.02983\n",
      "[281]\ttrain-rmse:0.029774\n",
      "[282]\ttrain-rmse:0.02971\n",
      "[283]\ttrain-rmse:0.029677\n",
      "[284]\ttrain-rmse:0.029627\n",
      "[285]\ttrain-rmse:0.029606\n",
      "[286]\ttrain-rmse:0.029587\n",
      "[287]\ttrain-rmse:0.029577\n",
      "[288]\ttrain-rmse:0.02955\n",
      "[289]\ttrain-rmse:0.029487\n",
      "[290]\ttrain-rmse:0.029463\n",
      "[291]\ttrain-rmse:0.029455\n",
      "[292]\ttrain-rmse:0.029418\n",
      "[293]\ttrain-rmse:0.029407\n",
      "[294]\ttrain-rmse:0.029364\n",
      "[295]\ttrain-rmse:0.029344\n",
      "[296]\ttrain-rmse:0.029336\n",
      "[297]\ttrain-rmse:0.029316\n",
      "[298]\ttrain-rmse:0.029291\n",
      "[299]\ttrain-rmse:0.029271\n",
      "[300]\ttrain-rmse:0.02922\n",
      "[301]\ttrain-rmse:0.029189\n",
      "[302]\ttrain-rmse:0.029156\n",
      "[303]\ttrain-rmse:0.029123\n",
      "[304]\ttrain-rmse:0.029101\n",
      "[305]\ttrain-rmse:0.029034\n",
      "[306]\ttrain-rmse:0.028993\n",
      "[307]\ttrain-rmse:0.028984\n",
      "[308]\ttrain-rmse:0.028943\n",
      "[309]\ttrain-rmse:0.028925\n",
      "[310]\ttrain-rmse:0.028913\n",
      "[311]\ttrain-rmse:0.028894\n",
      "[312]\ttrain-rmse:0.028886\n",
      "[313]\ttrain-rmse:0.028864\n",
      "[314]\ttrain-rmse:0.028822\n",
      "[315]\ttrain-rmse:0.028801\n",
      "[316]\ttrain-rmse:0.028772\n",
      "[317]\ttrain-rmse:0.028733\n",
      "[318]\ttrain-rmse:0.028714\n",
      "[319]\ttrain-rmse:0.028709\n",
      "[320]\ttrain-rmse:0.028688\n",
      "[321]\ttrain-rmse:0.028642\n",
      "[322]\ttrain-rmse:0.028632\n",
      "[323]\ttrain-rmse:0.028608\n",
      "[324]\ttrain-rmse:0.028587\n",
      "[325]\ttrain-rmse:0.028579\n",
      "[326]\ttrain-rmse:0.028546\n",
      "[327]\ttrain-rmse:0.028539\n",
      "[328]\ttrain-rmse:0.028534\n",
      "[329]\ttrain-rmse:0.028497\n",
      "[330]\ttrain-rmse:0.028465\n",
      "[331]\ttrain-rmse:0.028413\n",
      "[332]\ttrain-rmse:0.028406\n",
      "[333]\ttrain-rmse:0.028337\n",
      "[334]\ttrain-rmse:0.028316\n",
      "[335]\ttrain-rmse:0.028298\n",
      "[336]\ttrain-rmse:0.02827\n",
      "[337]\ttrain-rmse:0.028259\n",
      "[338]\ttrain-rmse:0.02823\n",
      "[339]\ttrain-rmse:0.028216\n",
      "[340]\ttrain-rmse:0.0282\n",
      "[341]\ttrain-rmse:0.028162\n",
      "[342]\ttrain-rmse:0.028148\n",
      "[343]\ttrain-rmse:0.028139\n",
      "[344]\ttrain-rmse:0.02809\n",
      "[345]\ttrain-rmse:0.028067\n",
      "[346]\ttrain-rmse:0.028061\n",
      "[347]\ttrain-rmse:0.028041\n",
      "[348]\ttrain-rmse:0.028029\n",
      "[349]\ttrain-rmse:0.028002\n",
      "[350]\ttrain-rmse:0.027995\n",
      "[351]\ttrain-rmse:0.027985\n",
      "[352]\ttrain-rmse:0.027977\n",
      "[353]\ttrain-rmse:0.027968\n",
      "[354]\ttrain-rmse:0.027939\n",
      "[355]\ttrain-rmse:0.027929\n",
      "[356]\ttrain-rmse:0.027872\n",
      "[357]\ttrain-rmse:0.02784\n",
      "[358]\ttrain-rmse:0.027832\n",
      "[359]\ttrain-rmse:0.027795\n",
      "[360]\ttrain-rmse:0.027783\n",
      "[361]\ttrain-rmse:0.02777\n",
      "[362]\ttrain-rmse:0.027763\n",
      "[363]\ttrain-rmse:0.027755\n",
      "[364]\ttrain-rmse:0.027744\n",
      "[365]\ttrain-rmse:0.027732\n",
      "[366]\ttrain-rmse:0.027707\n",
      "[367]\ttrain-rmse:0.027696\n",
      "[368]\ttrain-rmse:0.027669\n",
      "[369]\ttrain-rmse:0.02765\n",
      "[370]\ttrain-rmse:0.027601\n",
      "[371]\ttrain-rmse:0.027539\n",
      "[372]\ttrain-rmse:0.027496\n",
      "[373]\ttrain-rmse:0.027426\n",
      "[374]\ttrain-rmse:0.0274\n",
      "[375]\ttrain-rmse:0.027353\n",
      "[376]\ttrain-rmse:0.027337\n",
      "[377]\ttrain-rmse:0.027292\n",
      "[378]\ttrain-rmse:0.027246\n",
      "[379]\ttrain-rmse:0.027237\n",
      "[380]\ttrain-rmse:0.027224\n",
      "[381]\ttrain-rmse:0.027211\n",
      "[382]\ttrain-rmse:0.027204\n",
      "[383]\ttrain-rmse:0.027199\n",
      "[384]\ttrain-rmse:0.027192\n",
      "[385]\ttrain-rmse:0.027159\n",
      "[386]\ttrain-rmse:0.027119\n",
      "[387]\ttrain-rmse:0.027098\n",
      "[388]\ttrain-rmse:0.027055\n",
      "[389]\ttrain-rmse:0.027026\n",
      "[390]\ttrain-rmse:0.02702\n",
      "[391]\ttrain-rmse:0.026982\n",
      "[392]\ttrain-rmse:0.026972\n",
      "[393]\ttrain-rmse:0.026966\n",
      "[394]\ttrain-rmse:0.026952\n",
      "[395]\ttrain-rmse:0.026918\n",
      "[396]\ttrain-rmse:0.026904\n",
      "[397]\ttrain-rmse:0.026895\n",
      "[398]\ttrain-rmse:0.026888\n",
      "[399]\ttrain-rmse:0.026882\n",
      "[400]\ttrain-rmse:0.02687\n",
      "[401]\ttrain-rmse:0.026824\n",
      "[402]\ttrain-rmse:0.026809\n",
      "[403]\ttrain-rmse:0.026801\n",
      "[404]\ttrain-rmse:0.026794\n",
      "[405]\ttrain-rmse:0.026752\n",
      "[406]\ttrain-rmse:0.026707\n",
      "[407]\ttrain-rmse:0.0267\n",
      "[408]\ttrain-rmse:0.026693\n",
      "[409]\ttrain-rmse:0.026688\n",
      "[410]\ttrain-rmse:0.026617\n",
      "[411]\ttrain-rmse:0.026605\n",
      "[412]\ttrain-rmse:0.026593\n",
      "[413]\ttrain-rmse:0.026554\n",
      "[414]\ttrain-rmse:0.026549\n",
      "[415]\ttrain-rmse:0.026507\n",
      "[416]\ttrain-rmse:0.026466\n",
      "[417]\ttrain-rmse:0.026452\n",
      "[418]\ttrain-rmse:0.026403\n",
      "[419]\ttrain-rmse:0.026329\n",
      "[420]\ttrain-rmse:0.026322\n",
      "[421]\ttrain-rmse:0.026286\n",
      "[422]\ttrain-rmse:0.026256\n",
      "[423]\ttrain-rmse:0.02625\n",
      "[424]\ttrain-rmse:0.026243\n",
      "[425]\ttrain-rmse:0.026214\n",
      "[426]\ttrain-rmse:0.026207\n",
      "[427]\ttrain-rmse:0.026195\n",
      "[428]\ttrain-rmse:0.026178\n",
      "[429]\ttrain-rmse:0.026117\n",
      "[430]\ttrain-rmse:0.026091\n",
      "[431]\ttrain-rmse:0.026074\n",
      "[432]\ttrain-rmse:0.026044\n",
      "[433]\ttrain-rmse:0.02604\n",
      "[434]\ttrain-rmse:0.025986\n",
      "[435]\ttrain-rmse:0.025956\n",
      "[436]\ttrain-rmse:0.025926\n",
      "[437]\ttrain-rmse:0.025873\n",
      "[438]\ttrain-rmse:0.025847\n",
      "[439]\ttrain-rmse:0.025835\n",
      "[440]\ttrain-rmse:0.025826\n",
      "[441]\ttrain-rmse:0.025811\n",
      "[442]\ttrain-rmse:0.025805\n",
      "[443]\ttrain-rmse:0.025783\n",
      "[444]\ttrain-rmse:0.025774\n",
      "[445]\ttrain-rmse:0.025762\n",
      "[446]\ttrain-rmse:0.025751\n",
      "[447]\ttrain-rmse:0.025745\n",
      "[448]\ttrain-rmse:0.025715\n",
      "[449]\ttrain-rmse:0.025681\n",
      "[450]\ttrain-rmse:0.02567\n",
      "[451]\ttrain-rmse:0.025663\n",
      "[452]\ttrain-rmse:0.025645\n",
      "[453]\ttrain-rmse:0.025635\n",
      "[454]\ttrain-rmse:0.025611\n",
      "[455]\ttrain-rmse:0.025578\n",
      "[456]\ttrain-rmse:0.025566\n",
      "[457]\ttrain-rmse:0.02556\n",
      "[458]\ttrain-rmse:0.025519\n",
      "[459]\ttrain-rmse:0.025511\n",
      "[460]\ttrain-rmse:0.025505\n",
      "[461]\ttrain-rmse:0.025476\n",
      "[462]\ttrain-rmse:0.025437\n",
      "[463]\ttrain-rmse:0.025388\n",
      "[464]\ttrain-rmse:0.025381\n",
      "[465]\ttrain-rmse:0.02537\n",
      "[466]\ttrain-rmse:0.025364\n",
      "[467]\ttrain-rmse:0.025357\n",
      "[468]\ttrain-rmse:0.025317\n",
      "[469]\ttrain-rmse:0.025278\n",
      "[470]\ttrain-rmse:0.02527\n",
      "[471]\ttrain-rmse:0.025255\n",
      "[472]\ttrain-rmse:0.025247\n",
      "[473]\ttrain-rmse:0.025227\n",
      "[474]\ttrain-rmse:0.025207\n",
      "[475]\ttrain-rmse:0.025201\n",
      "[476]\ttrain-rmse:0.025196\n",
      "[477]\ttrain-rmse:0.025177\n",
      "[478]\ttrain-rmse:0.025168\n",
      "[479]\ttrain-rmse:0.025142\n",
      "[480]\ttrain-rmse:0.025109\n",
      "[481]\ttrain-rmse:0.025081\n",
      "[482]\ttrain-rmse:0.025057\n",
      "[483]\ttrain-rmse:0.025034\n",
      "[484]\ttrain-rmse:0.025015\n",
      "[485]\ttrain-rmse:0.025006\n",
      "[486]\ttrain-rmse:0.024998\n",
      "[487]\ttrain-rmse:0.024993\n",
      "[488]\ttrain-rmse:0.024913\n",
      "[489]\ttrain-rmse:0.024895\n",
      "[490]\ttrain-rmse:0.02489\n",
      "[491]\ttrain-rmse:0.02485\n",
      "[492]\ttrain-rmse:0.024841\n",
      "[493]\ttrain-rmse:0.024823\n",
      "[494]\ttrain-rmse:0.024805\n",
      "[495]\ttrain-rmse:0.024795\n",
      "[496]\ttrain-rmse:0.024778\n",
      "[497]\ttrain-rmse:0.024721\n",
      "[498]\ttrain-rmse:0.024707\n",
      "[499]\ttrain-rmse:0.024699\n",
      "[500]\ttrain-rmse:0.024681\n",
      "[501]\ttrain-rmse:0.024649\n",
      "[502]\ttrain-rmse:0.024621\n",
      "[503]\ttrain-rmse:0.02458\n",
      "[504]\ttrain-rmse:0.024572\n",
      "[505]\ttrain-rmse:0.024568\n",
      "[506]\ttrain-rmse:0.024531\n",
      "[507]\ttrain-rmse:0.024527\n",
      "[508]\ttrain-rmse:0.024479\n",
      "[509]\ttrain-rmse:0.02447\n",
      "[510]\ttrain-rmse:0.024436\n",
      "[511]\ttrain-rmse:0.024424\n",
      "[512]\ttrain-rmse:0.024409\n",
      "[513]\ttrain-rmse:0.024393\n",
      "[514]\ttrain-rmse:0.024358\n",
      "[515]\ttrain-rmse:0.024339\n",
      "[516]\ttrain-rmse:0.024311\n",
      "[517]\ttrain-rmse:0.024303\n",
      "[518]\ttrain-rmse:0.024298\n",
      "[519]\ttrain-rmse:0.024256\n",
      "[520]\ttrain-rmse:0.024248\n",
      "[521]\ttrain-rmse:0.02424\n",
      "[522]\ttrain-rmse:0.024222\n",
      "[523]\ttrain-rmse:0.024212\n",
      "[524]\ttrain-rmse:0.024207\n",
      "[525]\ttrain-rmse:0.024198\n",
      "[526]\ttrain-rmse:0.024193\n",
      "[527]\ttrain-rmse:0.024183\n",
      "[528]\ttrain-rmse:0.02414\n",
      "[529]\ttrain-rmse:0.024134\n",
      "[530]\ttrain-rmse:0.024127\n",
      "[531]\ttrain-rmse:0.024124\n",
      "[532]\ttrain-rmse:0.024091\n",
      "[533]\ttrain-rmse:0.024064\n",
      "[534]\ttrain-rmse:0.024042\n",
      "[535]\ttrain-rmse:0.024037\n",
      "[536]\ttrain-rmse:0.024004\n",
      "[537]\ttrain-rmse:0.023997\n",
      "[538]\ttrain-rmse:0.023994\n",
      "[539]\ttrain-rmse:0.023972\n",
      "[540]\ttrain-rmse:0.023958\n",
      "[541]\ttrain-rmse:0.023915\n",
      "[542]\ttrain-rmse:0.023911\n",
      "[543]\ttrain-rmse:0.023884\n",
      "[544]\ttrain-rmse:0.023844\n",
      "[545]\ttrain-rmse:0.023828\n",
      "[546]\ttrain-rmse:0.023824\n",
      "[547]\ttrain-rmse:0.023794\n",
      "[548]\ttrain-rmse:0.023744\n",
      "[549]\ttrain-rmse:0.023742\n",
      "[550]\ttrain-rmse:0.023723\n",
      "[551]\ttrain-rmse:0.0237\n",
      "[552]\ttrain-rmse:0.023693\n",
      "[553]\ttrain-rmse:0.023678\n",
      "[554]\ttrain-rmse:0.023642\n",
      "[555]\ttrain-rmse:0.023607\n",
      "[556]\ttrain-rmse:0.023601\n",
      "[557]\ttrain-rmse:0.023561\n",
      "[558]\ttrain-rmse:0.023552\n",
      "[559]\ttrain-rmse:0.023547\n",
      "[560]\ttrain-rmse:0.023512\n",
      "[561]\ttrain-rmse:0.023489\n",
      "[562]\ttrain-rmse:0.023464\n",
      "[563]\ttrain-rmse:0.023461\n",
      "[564]\ttrain-rmse:0.023457\n",
      "[565]\ttrain-rmse:0.02345\n",
      "[566]\ttrain-rmse:0.023447\n",
      "[567]\ttrain-rmse:0.023416\n",
      "[568]\ttrain-rmse:0.023386\n",
      "[569]\ttrain-rmse:0.023378\n",
      "[570]\ttrain-rmse:0.023373\n",
      "[571]\ttrain-rmse:0.023357\n",
      "[572]\ttrain-rmse:0.023351\n",
      "[573]\ttrain-rmse:0.023336\n",
      "[574]\ttrain-rmse:0.023325\n",
      "[575]\ttrain-rmse:0.02332\n",
      "[576]\ttrain-rmse:0.023291\n",
      "[577]\ttrain-rmse:0.023285\n",
      "[578]\ttrain-rmse:0.023274\n",
      "[579]\ttrain-rmse:0.023259\n",
      "[580]\ttrain-rmse:0.023254\n",
      "[581]\ttrain-rmse:0.023233\n",
      "[582]\ttrain-rmse:0.023228\n",
      "[583]\ttrain-rmse:0.0232\n",
      "[584]\ttrain-rmse:0.023184\n",
      "[585]\ttrain-rmse:0.023163\n",
      "[586]\ttrain-rmse:0.02314\n",
      "[587]\ttrain-rmse:0.023123\n",
      "[588]\ttrain-rmse:0.023085\n",
      "[589]\ttrain-rmse:0.023079\n",
      "[590]\ttrain-rmse:0.023058\n",
      "[591]\ttrain-rmse:0.023051\n",
      "[592]\ttrain-rmse:0.023014\n",
      "[593]\ttrain-rmse:0.023007\n",
      "[594]\ttrain-rmse:0.023005\n",
      "[595]\ttrain-rmse:0.023001\n",
      "[596]\ttrain-rmse:0.022991\n",
      "[597]\ttrain-rmse:0.022959\n",
      "[598]\ttrain-rmse:0.02293\n",
      "[599]\ttrain-rmse:0.022926\n",
      "[600]\ttrain-rmse:0.022906\n",
      "[601]\ttrain-rmse:0.022871\n",
      "[602]\ttrain-rmse:0.022867\n",
      "[603]\ttrain-rmse:0.02284\n",
      "[604]\ttrain-rmse:0.022823\n",
      "[605]\ttrain-rmse:0.02282\n",
      "[606]\ttrain-rmse:0.022788\n",
      "[607]\ttrain-rmse:0.02274\n",
      "[608]\ttrain-rmse:0.022737\n",
      "[609]\ttrain-rmse:0.022733\n",
      "[610]\ttrain-rmse:0.022724\n",
      "[611]\ttrain-rmse:0.0227\n",
      "[612]\ttrain-rmse:0.022694\n",
      "[613]\ttrain-rmse:0.02266\n",
      "[614]\ttrain-rmse:0.022654\n",
      "[615]\ttrain-rmse:0.022645\n",
      "[616]\ttrain-rmse:0.022639\n",
      "[617]\ttrain-rmse:0.022636\n",
      "[618]\ttrain-rmse:0.022601\n",
      "[619]\ttrain-rmse:0.022572\n",
      "[620]\ttrain-rmse:0.022553\n",
      "[621]\ttrain-rmse:0.022548\n",
      "[622]\ttrain-rmse:0.022521\n",
      "[623]\ttrain-rmse:0.022519\n",
      "[624]\ttrain-rmse:0.022509\n",
      "[625]\ttrain-rmse:0.022505\n",
      "[626]\ttrain-rmse:0.02248\n",
      "[627]\ttrain-rmse:0.022442\n",
      "[628]\ttrain-rmse:0.022426\n",
      "[629]\ttrain-rmse:0.022392\n",
      "[630]\ttrain-rmse:0.02237\n",
      "[631]\ttrain-rmse:0.022335\n",
      "[632]\ttrain-rmse:0.022307\n",
      "[633]\ttrain-rmse:0.022296\n",
      "[634]\ttrain-rmse:0.022267\n",
      "[635]\ttrain-rmse:0.022258\n",
      "[636]\ttrain-rmse:0.022253\n",
      "[637]\ttrain-rmse:0.022251\n",
      "[638]\ttrain-rmse:0.022241\n",
      "[639]\ttrain-rmse:0.022206\n",
      "[640]\ttrain-rmse:0.022202\n",
      "[641]\ttrain-rmse:0.022171\n",
      "[642]\ttrain-rmse:0.022165\n",
      "[643]\ttrain-rmse:0.022152\n",
      "[644]\ttrain-rmse:0.02214\n",
      "[645]\ttrain-rmse:0.022135\n",
      "[646]\ttrain-rmse:0.022131\n",
      "[647]\ttrain-rmse:0.02211\n",
      "[648]\ttrain-rmse:0.022107\n",
      "[649]\ttrain-rmse:0.022098\n",
      "[650]\ttrain-rmse:0.022073\n",
      "[651]\ttrain-rmse:0.022067\n",
      "[652]\ttrain-rmse:0.022064\n",
      "[653]\ttrain-rmse:0.022036\n",
      "[654]\ttrain-rmse:0.022013\n",
      "[655]\ttrain-rmse:0.021979\n",
      "[656]\ttrain-rmse:0.021948\n",
      "[657]\ttrain-rmse:0.021938\n",
      "[658]\ttrain-rmse:0.02191\n",
      "[659]\ttrain-rmse:0.021887\n",
      "[660]\ttrain-rmse:0.021878\n",
      "[661]\ttrain-rmse:0.021849\n",
      "[662]\ttrain-rmse:0.021846\n",
      "[663]\ttrain-rmse:0.02183\n",
      "[664]\ttrain-rmse:0.021819\n",
      "[665]\ttrain-rmse:0.021809\n",
      "[666]\ttrain-rmse:0.021785\n",
      "[667]\ttrain-rmse:0.021779\n",
      "[668]\ttrain-rmse:0.021774\n",
      "[669]\ttrain-rmse:0.021772\n",
      "[670]\ttrain-rmse:0.021743\n",
      "[671]\ttrain-rmse:0.021731\n",
      "[672]\ttrain-rmse:0.021694\n",
      "[673]\ttrain-rmse:0.021676\n",
      "[674]\ttrain-rmse:0.021669\n",
      "[675]\ttrain-rmse:0.021645\n",
      "[676]\ttrain-rmse:0.021631\n",
      "[677]\ttrain-rmse:0.021621\n",
      "[678]\ttrain-rmse:0.021578\n",
      "[679]\ttrain-rmse:0.021574\n",
      "[680]\ttrain-rmse:0.021563\n",
      "[681]\ttrain-rmse:0.021552\n",
      "[682]\ttrain-rmse:0.02154\n",
      "[683]\ttrain-rmse:0.021534\n",
      "[684]\ttrain-rmse:0.021512\n",
      "[685]\ttrain-rmse:0.021486\n",
      "[686]\ttrain-rmse:0.02148\n",
      "[687]\ttrain-rmse:0.02147\n",
      "[688]\ttrain-rmse:0.021451\n",
      "[689]\ttrain-rmse:0.021432\n",
      "[690]\ttrain-rmse:0.021424\n",
      "[691]\ttrain-rmse:0.021421\n",
      "[692]\ttrain-rmse:0.021417\n",
      "[693]\ttrain-rmse:0.021399\n",
      "[694]\ttrain-rmse:0.021394\n",
      "[695]\ttrain-rmse:0.021361\n",
      "[696]\ttrain-rmse:0.021339\n",
      "[697]\ttrain-rmse:0.021322\n",
      "[698]\ttrain-rmse:0.021319\n",
      "[699]\ttrain-rmse:0.021297\n",
      "[700]\ttrain-rmse:0.021289\n",
      "[701]\ttrain-rmse:0.021284\n",
      "[702]\ttrain-rmse:0.021257\n",
      "[703]\ttrain-rmse:0.021235\n",
      "[704]\ttrain-rmse:0.021229\n",
      "[705]\ttrain-rmse:0.021217\n",
      "[706]\ttrain-rmse:0.0212\n",
      "[707]\ttrain-rmse:0.021197\n",
      "[708]\ttrain-rmse:0.021189\n",
      "[709]\ttrain-rmse:0.021181\n",
      "[710]\ttrain-rmse:0.02116\n",
      "[711]\ttrain-rmse:0.02114\n",
      "[712]\ttrain-rmse:0.021132\n",
      "[713]\ttrain-rmse:0.021128\n",
      "[714]\ttrain-rmse:0.02111\n",
      "[715]\ttrain-rmse:0.02108\n",
      "[716]\ttrain-rmse:0.021069\n",
      "[717]\ttrain-rmse:0.021059\n",
      "[718]\ttrain-rmse:0.021041\n",
      "[719]\ttrain-rmse:0.021029\n",
      "[720]\ttrain-rmse:0.021021\n",
      "[721]\ttrain-rmse:0.021014\n",
      "[722]\ttrain-rmse:0.021008\n",
      "[723]\ttrain-rmse:0.021003\n",
      "[724]\ttrain-rmse:0.020998\n",
      "[725]\ttrain-rmse:0.020987\n",
      "[726]\ttrain-rmse:0.02096\n",
      "[727]\ttrain-rmse:0.020948\n",
      "[728]\ttrain-rmse:0.020937\n",
      "[729]\ttrain-rmse:0.02092\n",
      "[730]\ttrain-rmse:0.020916\n",
      "[731]\ttrain-rmse:0.02089\n",
      "[732]\ttrain-rmse:0.020862\n",
      "[733]\ttrain-rmse:0.020844\n",
      "[734]\ttrain-rmse:0.020841\n",
      "[735]\ttrain-rmse:0.020835\n",
      "[736]\ttrain-rmse:0.02083\n",
      "[737]\ttrain-rmse:0.020822\n",
      "[738]\ttrain-rmse:0.020815\n",
      "[739]\ttrain-rmse:0.020806\n",
      "[740]\ttrain-rmse:0.020803\n",
      "[741]\ttrain-rmse:0.020793\n",
      "[742]\ttrain-rmse:0.020767\n",
      "[743]\ttrain-rmse:0.02075\n",
      "[744]\ttrain-rmse:0.020736\n",
      "[745]\ttrain-rmse:0.020715\n",
      "[746]\ttrain-rmse:0.020711\n",
      "[747]\ttrain-rmse:0.02069\n",
      "[748]\ttrain-rmse:0.020687\n",
      "[749]\ttrain-rmse:0.020682\n",
      "[750]\ttrain-rmse:0.020669\n",
      "[751]\ttrain-rmse:0.020662\n",
      "[752]\ttrain-rmse:0.020658\n",
      "[753]\ttrain-rmse:0.02062\n",
      "[754]\ttrain-rmse:0.020617\n",
      "[755]\ttrain-rmse:0.020603\n",
      "[756]\ttrain-rmse:0.020597\n",
      "[757]\ttrain-rmse:0.020594\n",
      "[758]\ttrain-rmse:0.020588\n",
      "[759]\ttrain-rmse:0.020558\n",
      "[760]\ttrain-rmse:0.020552\n",
      "[761]\ttrain-rmse:0.020546\n",
      "[762]\ttrain-rmse:0.020535\n",
      "[763]\ttrain-rmse:0.020533\n",
      "[764]\ttrain-rmse:0.02053\n",
      "[765]\ttrain-rmse:0.020527\n",
      "[766]\ttrain-rmse:0.020524\n",
      "[767]\ttrain-rmse:0.020521\n",
      "[768]\ttrain-rmse:0.020518\n",
      "[769]\ttrain-rmse:0.020513\n",
      "[770]\ttrain-rmse:0.020493\n",
      "[771]\ttrain-rmse:0.020468\n",
      "[772]\ttrain-rmse:0.020465\n",
      "[773]\ttrain-rmse:0.020459\n",
      "[774]\ttrain-rmse:0.020451\n",
      "[775]\ttrain-rmse:0.020436\n",
      "[776]\ttrain-rmse:0.020432\n",
      "[777]\ttrain-rmse:0.020428\n",
      "[778]\ttrain-rmse:0.020421\n",
      "[779]\ttrain-rmse:0.020395\n",
      "[780]\ttrain-rmse:0.020392\n",
      "[781]\ttrain-rmse:0.020375\n",
      "[782]\ttrain-rmse:0.020372\n",
      "[783]\ttrain-rmse:0.020366\n",
      "[784]\ttrain-rmse:0.020363\n",
      "[785]\ttrain-rmse:0.020353\n",
      "[786]\ttrain-rmse:0.020345\n",
      "[787]\ttrain-rmse:0.020337\n",
      "[788]\ttrain-rmse:0.020333\n",
      "[789]\ttrain-rmse:0.020328\n",
      "[790]\ttrain-rmse:0.020322\n",
      "[791]\ttrain-rmse:0.020319\n",
      "[792]\ttrain-rmse:0.020316\n",
      "[793]\ttrain-rmse:0.020299\n",
      "[794]\ttrain-rmse:0.02029\n",
      "[795]\ttrain-rmse:0.020273\n",
      "[796]\ttrain-rmse:0.020265\n",
      "[797]\ttrain-rmse:0.02024\n",
      "[798]\ttrain-rmse:0.020233\n",
      "[799]\ttrain-rmse:0.020226\n",
      "[800]\ttrain-rmse:0.020219\n",
      "[801]\ttrain-rmse:0.020209\n",
      "[802]\ttrain-rmse:0.020191\n",
      "[803]\ttrain-rmse:0.020184\n",
      "[804]\ttrain-rmse:0.020171\n",
      "[805]\ttrain-rmse:0.020159\n",
      "[806]\ttrain-rmse:0.020156\n",
      "[807]\ttrain-rmse:0.020152\n",
      "[808]\ttrain-rmse:0.020137\n",
      "[809]\ttrain-rmse:0.020123\n",
      "[810]\ttrain-rmse:0.020119\n",
      "[811]\ttrain-rmse:0.020115\n",
      "[812]\ttrain-rmse:0.020091\n",
      "[813]\ttrain-rmse:0.020087\n",
      "[814]\ttrain-rmse:0.020082\n",
      "[815]\ttrain-rmse:0.020064\n",
      "[816]\ttrain-rmse:0.020062\n",
      "[817]\ttrain-rmse:0.020058\n",
      "[818]\ttrain-rmse:0.020043\n",
      "[819]\ttrain-rmse:0.020019\n",
      "[820]\ttrain-rmse:0.020013\n",
      "[821]\ttrain-rmse:0.02001\n",
      "[822]\ttrain-rmse:0.020005\n",
      "[823]\ttrain-rmse:0.019978\n",
      "[824]\ttrain-rmse:0.019971\n",
      "[825]\ttrain-rmse:0.019957\n",
      "[826]\ttrain-rmse:0.019954\n",
      "[827]\ttrain-rmse:0.019918\n",
      "[828]\ttrain-rmse:0.019889\n",
      "[829]\ttrain-rmse:0.019863\n",
      "[830]\ttrain-rmse:0.019839\n",
      "[831]\ttrain-rmse:0.01983\n",
      "[832]\ttrain-rmse:0.019828\n",
      "[833]\ttrain-rmse:0.019824\n",
      "[834]\ttrain-rmse:0.01982\n",
      "[835]\ttrain-rmse:0.01979\n",
      "[836]\ttrain-rmse:0.019786\n",
      "[837]\ttrain-rmse:0.019778\n",
      "[838]\ttrain-rmse:0.019764\n",
      "[839]\ttrain-rmse:0.01976\n",
      "[840]\ttrain-rmse:0.019756\n",
      "[841]\ttrain-rmse:0.019739\n",
      "[842]\ttrain-rmse:0.019737\n",
      "[843]\ttrain-rmse:0.019731\n",
      "[844]\ttrain-rmse:0.019712\n",
      "[845]\ttrain-rmse:0.0197\n",
      "[846]\ttrain-rmse:0.019678\n",
      "[847]\ttrain-rmse:0.019667\n",
      "[848]\ttrain-rmse:0.019663\n",
      "[849]\ttrain-rmse:0.019658\n",
      "[850]\ttrain-rmse:0.019654\n",
      "[851]\ttrain-rmse:0.019639\n",
      "[852]\ttrain-rmse:0.019613\n",
      "[853]\ttrain-rmse:0.019608\n",
      "[854]\ttrain-rmse:0.019604\n",
      "[855]\ttrain-rmse:0.0196\n",
      "[856]\ttrain-rmse:0.01958\n",
      "[857]\ttrain-rmse:0.019575\n",
      "[858]\ttrain-rmse:0.019568\n",
      "[859]\ttrain-rmse:0.019554\n",
      "[860]\ttrain-rmse:0.019539\n",
      "[861]\ttrain-rmse:0.019523\n",
      "[862]\ttrain-rmse:0.019514\n",
      "[863]\ttrain-rmse:0.019501\n",
      "[864]\ttrain-rmse:0.019497\n",
      "[865]\ttrain-rmse:0.019492\n",
      "[866]\ttrain-rmse:0.019489\n",
      "[867]\ttrain-rmse:0.019477\n",
      "[868]\ttrain-rmse:0.019471\n",
      "[869]\ttrain-rmse:0.019449\n",
      "[870]\ttrain-rmse:0.019444\n",
      "[871]\ttrain-rmse:0.019442\n",
      "[872]\ttrain-rmse:0.01943\n",
      "[873]\ttrain-rmse:0.019418\n",
      "[874]\ttrain-rmse:0.019408\n",
      "[875]\ttrain-rmse:0.019398\n",
      "[876]\ttrain-rmse:0.019382\n",
      "[877]\ttrain-rmse:0.019379\n",
      "[878]\ttrain-rmse:0.019357\n",
      "[879]\ttrain-rmse:0.019339\n",
      "[880]\ttrain-rmse:0.019334\n",
      "[881]\ttrain-rmse:0.019331\n",
      "[882]\ttrain-rmse:0.01932\n",
      "[883]\ttrain-rmse:0.019306\n",
      "[884]\ttrain-rmse:0.019302\n",
      "[885]\ttrain-rmse:0.019298\n",
      "[886]\ttrain-rmse:0.019289\n",
      "[887]\ttrain-rmse:0.019287\n",
      "[888]\ttrain-rmse:0.019284\n",
      "[889]\ttrain-rmse:0.01926\n",
      "[890]\ttrain-rmse:0.019252\n",
      "[891]\ttrain-rmse:0.019243\n",
      "[892]\ttrain-rmse:0.019216\n",
      "[893]\ttrain-rmse:0.019199\n",
      "[894]\ttrain-rmse:0.019189\n",
      "[895]\ttrain-rmse:0.019167\n",
      "[896]\ttrain-rmse:0.019148\n",
      "[897]\ttrain-rmse:0.019143\n",
      "[898]\ttrain-rmse:0.019137\n",
      "[899]\ttrain-rmse:0.01913\n",
      "[900]\ttrain-rmse:0.019115\n",
      "[901]\ttrain-rmse:0.019111\n",
      "[902]\ttrain-rmse:0.019106\n",
      "[903]\ttrain-rmse:0.019103\n",
      "[904]\ttrain-rmse:0.019081\n",
      "[905]\ttrain-rmse:0.01906\n",
      "[906]\ttrain-rmse:0.019046\n",
      "[907]\ttrain-rmse:0.019038\n",
      "[908]\ttrain-rmse:0.019018\n",
      "[909]\ttrain-rmse:0.019016\n",
      "[910]\ttrain-rmse:0.019003\n",
      "[911]\ttrain-rmse:0.01899\n",
      "[912]\ttrain-rmse:0.018972\n",
      "[913]\ttrain-rmse:0.018962\n",
      "[914]\ttrain-rmse:0.018947\n",
      "[915]\ttrain-rmse:0.018932\n",
      "[916]\ttrain-rmse:0.018914\n",
      "[917]\ttrain-rmse:0.018897\n",
      "[918]\ttrain-rmse:0.018882\n",
      "[919]\ttrain-rmse:0.018878\n",
      "[920]\ttrain-rmse:0.018866\n",
      "[921]\ttrain-rmse:0.018853\n",
      "[922]\ttrain-rmse:0.01885\n",
      "[923]\ttrain-rmse:0.018847\n",
      "[924]\ttrain-rmse:0.018839\n",
      "[925]\ttrain-rmse:0.018824\n",
      "[926]\ttrain-rmse:0.018819\n",
      "[927]\ttrain-rmse:0.018805\n",
      "[928]\ttrain-rmse:0.018802\n",
      "[929]\ttrain-rmse:0.018799\n",
      "[930]\ttrain-rmse:0.018791\n",
      "[931]\ttrain-rmse:0.018786\n",
      "[932]\ttrain-rmse:0.018783\n",
      "[933]\ttrain-rmse:0.01878\n",
      "[934]\ttrain-rmse:0.018776\n",
      "[935]\ttrain-rmse:0.018767\n",
      "[936]\ttrain-rmse:0.018747\n",
      "[937]\ttrain-rmse:0.018744\n",
      "[938]\ttrain-rmse:0.018737\n",
      "[939]\ttrain-rmse:0.018729\n",
      "[940]\ttrain-rmse:0.01871\n",
      "[941]\ttrain-rmse:0.018701\n",
      "[942]\ttrain-rmse:0.018698\n",
      "[943]\ttrain-rmse:0.018685\n",
      "[944]\ttrain-rmse:0.018675\n",
      "[945]\ttrain-rmse:0.018673\n",
      "[946]\ttrain-rmse:0.018671\n",
      "[947]\ttrain-rmse:0.018662\n",
      "[948]\ttrain-rmse:0.018647\n",
      "[949]\ttrain-rmse:0.01864\n",
      "[950]\ttrain-rmse:0.018639\n",
      "[951]\ttrain-rmse:0.018624\n",
      "[952]\ttrain-rmse:0.018617\n",
      "[953]\ttrain-rmse:0.018613\n",
      "[954]\ttrain-rmse:0.018609\n",
      "[955]\ttrain-rmse:0.018599\n",
      "[956]\ttrain-rmse:0.018591\n",
      "[957]\ttrain-rmse:0.018588\n",
      "[958]\ttrain-rmse:0.018574\n",
      "[959]\ttrain-rmse:0.018554\n",
      "[960]\ttrain-rmse:0.018548\n",
      "[961]\ttrain-rmse:0.018544\n",
      "[962]\ttrain-rmse:0.018524\n",
      "[963]\ttrain-rmse:0.018521\n",
      "[964]\ttrain-rmse:0.018516\n",
      "[965]\ttrain-rmse:0.018496\n",
      "[966]\ttrain-rmse:0.018492\n",
      "[967]\ttrain-rmse:0.018485\n",
      "[968]\ttrain-rmse:0.018479\n",
      "[969]\ttrain-rmse:0.018477\n",
      "[970]\ttrain-rmse:0.018467\n",
      "[971]\ttrain-rmse:0.018459\n",
      "[972]\ttrain-rmse:0.018455\n",
      "[973]\ttrain-rmse:0.018453\n",
      "[974]\ttrain-rmse:0.01845\n",
      "[975]\ttrain-rmse:0.018445\n",
      "[976]\ttrain-rmse:0.018437\n",
      "[977]\ttrain-rmse:0.018429\n",
      "[978]\ttrain-rmse:0.018425\n",
      "[979]\ttrain-rmse:0.01842\n",
      "[980]\ttrain-rmse:0.018418\n",
      "[981]\ttrain-rmse:0.018413\n",
      "[982]\ttrain-rmse:0.018405\n",
      "[983]\ttrain-rmse:0.018402\n",
      "[984]\ttrain-rmse:0.018399\n",
      "[985]\ttrain-rmse:0.018393\n",
      "[986]\ttrain-rmse:0.018384\n",
      "[987]\ttrain-rmse:0.018372\n",
      "[988]\ttrain-rmse:0.018362\n",
      "[989]\ttrain-rmse:0.018358\n",
      "[990]\ttrain-rmse:0.018355\n",
      "[991]\ttrain-rmse:0.01835\n",
      "[992]\ttrain-rmse:0.018334\n",
      "[993]\ttrain-rmse:0.018324\n",
      "[994]\ttrain-rmse:0.018314\n",
      "[995]\ttrain-rmse:0.018311\n",
      "[996]\ttrain-rmse:0.018305\n",
      "[997]\ttrain-rmse:0.0183\n",
      "[998]\ttrain-rmse:0.018281\n",
      "[999]\ttrain-rmse:0.018277\n",
      "[1000]\ttrain-rmse:0.018273\n",
      "[1001]\ttrain-rmse:0.018268\n",
      "[1002]\ttrain-rmse:0.018264\n",
      "[1003]\ttrain-rmse:0.018247\n",
      "[1004]\ttrain-rmse:0.018238\n",
      "[1005]\ttrain-rmse:0.018229\n",
      "[1006]\ttrain-rmse:0.018225\n",
      "[1007]\ttrain-rmse:0.018222\n",
      "[1008]\ttrain-rmse:0.018216\n",
      "[1009]\ttrain-rmse:0.018201\n",
      "[1010]\ttrain-rmse:0.018194\n",
      "[1011]\ttrain-rmse:0.018183\n",
      "[1012]\ttrain-rmse:0.018159\n",
      "[1013]\ttrain-rmse:0.018143\n",
      "[1014]\ttrain-rmse:0.01814\n",
      "[1015]\ttrain-rmse:0.018132\n",
      "[1016]\ttrain-rmse:0.01813\n",
      "[1017]\ttrain-rmse:0.018116\n",
      "[1018]\ttrain-rmse:0.018107\n",
      "[1019]\ttrain-rmse:0.018101\n",
      "[1020]\ttrain-rmse:0.018094\n",
      "[1021]\ttrain-rmse:0.018091\n",
      "[1022]\ttrain-rmse:0.018088\n",
      "[1023]\ttrain-rmse:0.018078\n",
      "[1024]\ttrain-rmse:0.018074\n",
      "[1025]\ttrain-rmse:0.01807\n",
      "[1026]\ttrain-rmse:0.018048\n",
      "[1027]\ttrain-rmse:0.018037\n",
      "[1028]\ttrain-rmse:0.018027\n",
      "[1029]\ttrain-rmse:0.018004\n",
      "[1030]\ttrain-rmse:0.018\n",
      "[1031]\ttrain-rmse:0.017994\n",
      "[1032]\ttrain-rmse:0.017992\n",
      "[1033]\ttrain-rmse:0.017975\n",
      "[1034]\ttrain-rmse:0.017972\n",
      "[1035]\ttrain-rmse:0.017971\n",
      "[1036]\ttrain-rmse:0.01796\n",
      "[1037]\ttrain-rmse:0.017953\n",
      "[1038]\ttrain-rmse:0.017948\n",
      "[1039]\ttrain-rmse:0.017945\n",
      "[1040]\ttrain-rmse:0.01794\n",
      "[1041]\ttrain-rmse:0.01792\n",
      "[1042]\ttrain-rmse:0.017908\n",
      "[1043]\ttrain-rmse:0.017905\n",
      "[1044]\ttrain-rmse:0.017902\n",
      "[1045]\ttrain-rmse:0.017899\n",
      "[1046]\ttrain-rmse:0.017888\n",
      "[1047]\ttrain-rmse:0.017882\n",
      "[1048]\ttrain-rmse:0.01788\n",
      "[1049]\ttrain-rmse:0.01787\n",
      "[1050]\ttrain-rmse:0.017866\n",
      "[1051]\ttrain-rmse:0.017864\n",
      "[1052]\ttrain-rmse:0.017857\n",
      "[1053]\ttrain-rmse:0.017854\n",
      "[1054]\ttrain-rmse:0.01785\n",
      "[1055]\ttrain-rmse:0.017842\n",
      "[1056]\ttrain-rmse:0.017839\n",
      "[1057]\ttrain-rmse:0.017835\n",
      "[1058]\ttrain-rmse:0.017828\n",
      "[1059]\ttrain-rmse:0.017824\n",
      "[1060]\ttrain-rmse:0.017823\n",
      "[1061]\ttrain-rmse:0.017819\n",
      "[1062]\ttrain-rmse:0.017815\n",
      "[1063]\ttrain-rmse:0.017798\n",
      "[1064]\ttrain-rmse:0.017789\n",
      "[1065]\ttrain-rmse:0.017785\n",
      "[1066]\ttrain-rmse:0.01778\n",
      "[1067]\ttrain-rmse:0.017776\n",
      "[1068]\ttrain-rmse:0.017774\n",
      "[1069]\ttrain-rmse:0.017761\n",
      "[1070]\ttrain-rmse:0.017752\n",
      "[1071]\ttrain-rmse:0.017747\n",
      "[1072]\ttrain-rmse:0.017742\n",
      "[1073]\ttrain-rmse:0.017726\n",
      "[1074]\ttrain-rmse:0.017722\n",
      "[1075]\ttrain-rmse:0.017716\n",
      "[1076]\ttrain-rmse:0.017714\n",
      "[1077]\ttrain-rmse:0.017712\n",
      "[1078]\ttrain-rmse:0.017708\n",
      "[1079]\ttrain-rmse:0.017705\n",
      "[1080]\ttrain-rmse:0.017694\n",
      "[1081]\ttrain-rmse:0.017691\n",
      "[1082]\ttrain-rmse:0.017678\n",
      "[1083]\ttrain-rmse:0.017672\n",
      "[1084]\ttrain-rmse:0.017668\n",
      "[1085]\ttrain-rmse:0.017655\n",
      "[1086]\ttrain-rmse:0.017647\n",
      "[1087]\ttrain-rmse:0.017643\n",
      "[1088]\ttrain-rmse:0.017639\n",
      "[1089]\ttrain-rmse:0.017636\n",
      "[1090]\ttrain-rmse:0.017629\n",
      "[1091]\ttrain-rmse:0.017617\n",
      "[1092]\ttrain-rmse:0.017614\n",
      "[1093]\ttrain-rmse:0.017607\n",
      "[1094]\ttrain-rmse:0.017596\n",
      "[1095]\ttrain-rmse:0.017576\n",
      "[1096]\ttrain-rmse:0.017569\n",
      "[1097]\ttrain-rmse:0.017564\n",
      "[1098]\ttrain-rmse:0.017552\n",
      "[1099]\ttrain-rmse:0.017545\n",
      "[1100]\ttrain-rmse:0.017539\n",
      "[1101]\ttrain-rmse:0.017534\n",
      "[1102]\ttrain-rmse:0.017531\n",
      "[1103]\ttrain-rmse:0.017521\n",
      "[1104]\ttrain-rmse:0.017501\n",
      "[1105]\ttrain-rmse:0.0175\n",
      "[1106]\ttrain-rmse:0.017497\n",
      "[1107]\ttrain-rmse:0.017492\n",
      "[1108]\ttrain-rmse:0.017475\n",
      "[1109]\ttrain-rmse:0.017465\n",
      "[1110]\ttrain-rmse:0.01745\n",
      "[1111]\ttrain-rmse:0.017445\n",
      "[1112]\ttrain-rmse:0.017443\n",
      "[1113]\ttrain-rmse:0.017439\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-700c0c4c3230>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBoost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-bb6ba8c57944>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m     13\u001b[0m                  'subsample': 0.7}\n\u001b[1;32m     14\u001b[0m         \u001b[0mnum_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevallist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Result on validation data: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/itba/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/itba/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/itba/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = XGBoost(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MÃ©trica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left(\\frac{\\hat{y}_i - y_i}{y_i}\\right)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if log_output:\n",
    "    y_pred_train = np.exp(model.predict(X_train, verbose=1)*max_log_y)\n",
    "    y_pred = np.exp(model.predict(X_val, verbose=1)*max_log_y)\n",
    "    y_pred_test = np.exp(model.predict(X_test, verbose=1)*max_log_y)\n",
    "else:\n",
    "    y_pred_train = model.predict(X_train, verbose=1)*y_std + y_mean\n",
    "    y_pred = model.predict(X_val, verbose=1)*y_std + y_mean\n",
    "    y_pred_test = model.predict(X_test, verbose=1)*y_std + y_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "np.sqrt((((df_train['Sales'].values - y_pred_train)/df_train['Sales'].values)**2).sum()/len(y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ValidaciÃ³n\n",
    "np.sqrt((((df_val['Sales'].values - y_pred)/df_val['Sales'].values)**2).sum()/len(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sample_csv = pd.read_csv('dataset/rossmann/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_mean = {}\n",
    "for store, g_df in df.groupby('Store'):\n",
    "    stores_mean[store] = g_df[g_df['Sales'] > 0]['Sales'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Sales'] = df_test['Store'].apply(stores_mean.get)\n",
    "df_test.loc[df_test['Open'] == 0, 'Sales'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[['Store', 'Sales']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[df_test['Open'] == 0][['Store', 'Sales']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_csv['Sales'] = df_test['Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_csv.to_csv(f'submision_baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sumbit a la competiciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_csv = pd.read_csv('dataset/rossmann/sample_submission.csv')\n",
    "sample_csv['Sales'] = y_pred_test\n",
    "sample_csv.head()\n",
    "\n",
    "sample_csv.to_csv(f'submision_{log_output}-{min_child_samples}-{n_estimators}-{learning_rate}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
